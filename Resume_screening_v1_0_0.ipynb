{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sreeyanaidu/resume-screening/blob/main/Resume_screening_v1_0_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFFkb4wwDgmc",
        "outputId": "b6e5b690-07a7-44b9-c2a9-f60992651bef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - accuracy: 0.8523 - loss: 0.6556 - val_accuracy: 0.9896 - val_loss: 0.3017\n",
            "Epoch 2/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9752 - loss: 0.3182 - val_accuracy: 0.9896 - val_loss: 0.0553\n",
            "Epoch 3/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 965ms/step - accuracy: 0.9946 - loss: 0.0316 - val_accuracy: 1.0000 - val_loss: 0.0029\n",
            "Epoch 4/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 0.0011\n",
            "Epoch 5/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 951ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 1.0000 - val_loss: 6.6998e-04\n",
            "Epoch 6/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 1.0000 - val_loss: 4.5513e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 972ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 1.0000 - val_loss: 3.2739e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 959ms/step - accuracy: 1.0000 - loss: 7.5542e-04 - val_accuracy: 1.0000 - val_loss: 2.4532e-04\n",
            "Epoch 9/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 5.4826e-04 - val_accuracy: 1.0000 - val_loss: 1.8812e-04\n",
            "Epoch 10/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 4.4699e-04 - val_accuracy: 1.0000 - val_loss: 1.4621e-04\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 194ms/step - accuracy: 1.0000 - loss: 1.4467e-04\n",
            "Test Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/UpdatedResumeDataSet.csv')\n",
        "\n",
        "# Clean text function\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    return text.lower()\n",
        "\n",
        "df['cleaned_resume'] = df['Resume'].apply(clean_text)\n",
        "\n",
        "# Convert job categories into binary outcome (Accepted = 1, Rejected = 0)\n",
        "accepted_categories = [\"Data Science\", \"Software Engineer\", \"Machine Learning Engineer\", \"Cyber Security\", \"AI Engineer\"]\n",
        "df['accepted'] = df['Category'].apply(lambda x: 1 if x in accepted_categories else 0)\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_resume'], df['accepted'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(num_words=10000)  # Increased vocabulary size\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Padding\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=500)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=500)\n",
        "\n",
        "# Handling Imbalanced Data\n",
        "class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "# Build LSTM Model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=10000, output_dim=128, input_length=500),  # Increased vocab size\n",
        "    Dropout(0.3),  # More dropout to prevent overfitting\n",
        "    LSTM(100, return_sequences=True),  # First LSTM layer\n",
        "    LSTM(50),  # Second LSTM layer\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n",
        "\n",
        "# Compile Model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#Train Model\n",
        "model.fit(X_train_pad, y_train, epochs=10, batch_size=32, validation_data=(X_test_pad, y_test), class_weight=class_weights)\n",
        "\n",
        "# Evaluate Model\n",
        "loss, acc = model.evaluate(X_test_pad, y_test)\n",
        "print(\"Test Accuracy:\", acc)\n",
        "\n",
        "# Function to Predict Resume Acceptance\n",
        "def predict_acceptance(resume_text):\n",
        "    resume_text = clean_text(resume_text)\n",
        "    seq = tokenizer.texts_to_sequences([resume_text])\n",
        "    pad = pad_sequences(seq, maxlen=500)\n",
        "    pred = model.predict(pad)[0][0]\n",
        "    return \"Accepted \" if pred > 0.5 else \"Rejected\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFF5aI70DnFU",
        "outputId": "33138e31-7e00-412f-f7e9-6085b691d5f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 1s/step - accuracy: 0.2128 - loss: 3.1977 - val_accuracy: 0.4093 - val_loss: 3.0841\n",
            "Epoch 2/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - accuracy: 0.3565 - loss: 2.9517 - val_accuracy: 0.5544 - val_loss: 2.6204\n",
            "Epoch 3/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.5081 - loss: 2.3952 - val_accuracy: 0.6269 - val_loss: 1.8763\n",
            "Epoch 4/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.5779 - loss: 1.7430 - val_accuracy: 0.6425 - val_loss: 1.3832\n",
            "Epoch 5/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 1s/step - accuracy: 0.6946 - loss: 1.3094 - val_accuracy: 0.7876 - val_loss: 1.0423\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - accuracy: 0.7906 - loss: 1.0443\n",
            "Test Accuracy: 0.787564754486084\n",
            "Resume: I am a software engineer with skills in Java, Python, and web development.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 491ms/step\n",
            "Predicted Category: Data Science\n",
            "------------------------------\n",
            "Resume: Digital marketing expert skilled in SEO and social media campaigns.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
            "Predicted Category: HR\n",
            "------------------------------\n",
            "Resume: Cybersecurity analyst with experience in penetration testing and network security.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
            "Predicted Category: Web Designing\n",
            "------------------------------\n",
            "Resume: Experienced data scientist working with Python, deep learning, and AI.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
            "Predicted Category: Web Designing\n",
            "------------------------------\n",
            "Resume: HR professional with a strong background in recruitment and employee relations.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
            "Predicted Category: HR\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('/content/UpdatedResumeDataSet.csv')\n",
        "\n",
        "# Clean text function\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    return text.lower()\n",
        "\n",
        "df['cleaned_resume'] = df['Resume'].apply(clean_text)  # Apply text cleaning\n",
        "\n",
        "# Encode job categories into numbers\n",
        "encoder = LabelEncoder()\n",
        "df['category_encoded'] = encoder.fit_transform(df['Category'])\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_resume'], df['category_encoded'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(num_words=5000)  # Limit vocab size\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Padding\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=500)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=500)\n",
        "\n",
        "# Number of categories\n",
        "num_classes = len(encoder.classes_)\n",
        "\n",
        "# Build Model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=5000, output_dim=128, input_length=500))  # Word embeddings\n",
        "model.add(Dropout(0.2))  # Prevent overfitting\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))  # LSTM layer\n",
        "model.add(Dense(num_classes, activation='softmax'))  # Output layer\n",
        "\n",
        "# Compile Model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train Model\n",
        "model.fit(X_train_pad, y_train, epochs=5, batch_size=32, validation_data=(X_test_pad, y_test))\n",
        "\n",
        "# Evaluate Model\n",
        "loss, acc = model.evaluate(X_test_pad, y_test)\n",
        "print(\"Test Accuracy:\", acc)\n",
        "\n",
        "# Function to Predict Job Category\n",
        "def predict_job(resume_text):\n",
        "    resume_text = clean_text(resume_text)\n",
        "    seq = tokenizer.texts_to_sequences([resume_text])\n",
        "    pad = pad_sequences(seq, maxlen=500)\n",
        "    pred = model.predict(pad)\n",
        "    return encoder.inverse_transform([np.argmax(pred)])[0]\n",
        "\n",
        "# Example Predictions\n",
        "sample_resumes = [\n",
        "    \"I am a software engineer with skills in Java, Python, and web development.\",\n",
        "    \"Digital marketing expert skilled in SEO and social media campaigns.\",\n",
        "    \"Cybersecurity analyst with experience in penetration testing and network security.\",\n",
        "    \"Experienced data scientist working with Python, deep learning, and AI.\",\n",
        "    \"HR professional with a strong background in recruitment and employee relations.\"\n",
        "]\n",
        "\n",
        "for resume in sample_resumes:\n",
        "    print(\"Resume:\", resume)\n",
        "    print(\"Predicted Category:\", predict_job(resume))\n",
        "    print(\"-\" * 30)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlqjvoFALSI3Bi4KQvT1JZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}